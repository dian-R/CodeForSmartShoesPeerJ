#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar 10 15:53:44 2019
@author: TGK Dian
THis is a kick ass smart shoes master program.
Start from reading the calibrated data
Filtration, normalizing, window cutting, feature extraction, ML program
"""

import scipy
from scipy import signal
import scipy.signal as sig
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
import time
start_time = time.time()
#%%

def instepfeature(window):
    "I'm still trying to figure out the order, but in this stage the whole idea is"
    "...."
    def stepcounting(x):
        "sumyl dosen't mean any about LEFT, it is just a sum of halfwindow, it could be L or R"
        A = x.max(axis = 1)
        A[A<1] = 0       #!!!!!!!!!!!This threshod needs discussing!!!!!!!!!!!!
        count = 1
        start = 0
        step = []
        for i in range(len(A)-1):
            if A[i] == 0:
                if A[i+1] == 0:
                    continue
                else:
                    count = count + 1
                    start = i
            else:
                if A[i+1] != 0:
                    continue
                else:
                    stop = i
                    t = stop - start
                    if A[start] == 0:
                            if t > 10:
                                step.append(x[start:stop+1])    
        return step
    "If in tht step has more tan two peaks, the difference between the last one and the first one"
    def avepeakdiff(step):
        peakdiff = []
        for each in step:
            maxinstep = each.max(axis=1)
            peaks, properties = scipy.signal.find_peaks(maxinstep,distance = 10,width = 10)
            peaknum = len(peaks)
            if peaknum > 1:
                peakdiff.append(maxinstep[peaks[peaknum-1]]-maxinstep[peaks[0]])
        return np.mean(peakdiff)
            
    yl = np.array(window[:,0:7])#?
    yr = np.array(window[:,7:14])#? 
    Lstep = stepcounting(yl)
    Rstep = stepcounting(yr)
    if (avepeakdiff(Lstep)+avepeakdiff(Rstep))/2  :
        inst = (avepeakdiff(Lstep)+avepeakdiff(Rstep))/2  
    else:
        inst = 0
    return inst   

def inerpeakinfo(window):
    def inersensorpeakinfo(single):

#==============================================================================
#         "|||NOTICE|||"
#         "The 3 functions below maybe not all neccessary."
#         "Have to discuess: a. which feature should use to identify a prominence. How to use for different fitures"
#         "by 1. distance to the privious one. 2. The top megnitude 3. width"
#         "b. How to identify the index of each item."
#         "|||NOTICE END|||"
#==============================================================================
        isf = [] #isf = inner sensor features
        peaks, properties = scipy.signal.find_peaks(single, distance = 30,prominence=2*np.std(single),width=0,rel_height=0.8)
#        peaks, properties = scipy.signal.find_peaks(single,prominence = 200) 
#        peaks, properties = scipy.signal.find_peaks(single, width=20)
        "Total # of peaks"
        ft_pk_no = len(peaks)
        "Average distans between peaks"
        ft_pk2pk_dst = np.average(np.diff(peaks))
        ft_pk2pk_std = np.std(np.diff(peaks))
        "Average/std of magnitudes"
        ft_mg_ave = np.average(properties['prominences'])
        ft_mg_std = np.std(properties['prominences'])
        "Average/std of width"
        ft_wd_ave = np.average(properties['widths'])
        ft_wd_std = np.std(properties['widths']) 
        ft_mg_ave = np.average(properties['prominences'])
        # If we adjust the window length 
        #Change this into 3
        if ft_pk_no > 1:
            isf = [ft_pk_no,ft_pk2pk_dst,ft_pk2pk_std,ft_mg_ave,ft_mg_std,ft_wd_ave,ft_wd_std]
        elif ft_pk_no == 1:
            isf = [ft_pk_no,0,0,ft_mg_ave,ft_mg_std,ft_wd_ave,ft_wd_std]
        else:
            isf = [0,0,0,0,0,0,0]
        return isf
    inerpeakinfo = []
    for column in window.T:
        inerpeakinfo.append(inersensorpeakinfo(column))
    return inerpeakinfo

def sensorgeneral (window):
    std = []
    ave = []
    mid = []
    maximum = []
    for columns in window.T:
        y = list(filter(lambda a: a != 0.0, columns)) #Ramove zeros 
        #y = columns
        ave.append(np.mean(y))
        std.append(np.std(y))  
        mid.append(np.median(y))  
        maximum.append(np.max(y))
    return [ave,std,mid,maximum]

"The difference Anterior & Posterior mean of max in one step"   

def anterposter (window):
    anterL = window[:,3:7].max(axis=1) # A column that all element is the biggest of that row in [:,3,7] 
    posterL = window[:,0]
    diffl = np.mean(anterL)-np.mean(posterL)
    anterR = window[:,10:14].max(axis=1) 
    posterR = window[:,7]
    diffr = np.mean(anterR)-np.mean(posterR)
    avediff = (diffl+diffr)/2
    correlationL = np.corrcoef(anterL,posterL)[0][1]
    correlationR = np.corrcoef(anterR,posterR)[0][1]
    return (avediff,correlationL,correlationR)

"The difference latterior & middle mean of max in one step" 
#Should it be the exact numbers or a diff of two averages?
def lattomid (window) :
    latteriorL = window[:,2]
    midL = window[:,4]
    latteriorR = window[:,9]
    midR = window[:,11]
    avediff = ((np.mean(latteriorL)-np.mean(midL))+(np.mean(latteriorR)-np.mean(midR)))/2
    correlationL = np.corrcoef(latteriorL,midL)[0][1]
    correlationR = np.corrcoef(latteriorR,midR)[0][1]
    return (avediff,correlationL,correlationR)
    
def overlappingrate (window): #Here the input is window.
    window[window<5] = 0 #This threshod needs disscusion!!!
    count = 0 #Here the count is for counting overlapping rows
    for each in window:
        if any(each[0:7]) != 0 and any(each[7:14]) != 0:
            count += 1
    return count/len(window)

def fft(window):
    t = window.sum(axis = 1)
    fft_trans = np.abs(np.fft.fft(t))
    dc = fft_trans[1]
    freq_spectrum = fft_trans[1:int(np.floor(len(t) * 1.0 / 2)) + 1]
    freq_sum_ = np.sum(freq_spectrum)
    pr_freq = freq_spectrum * 1.0 / freq_sum_
    entropy = -1 * np.sum([np.log2(p) * p for p in pr_freq])
    return (dc, np.mean(freq_spectrum), np.std(freq_spectrum), entropy, np.sum(freq_spectrum ** 2) / len(freq_spectrum)) 
#%%

files = []

labels = ['upstairs', 'downstairs', 'jog', 'nonlocal', 'run', 'sit', 'stand', 'walkF', 'walkN', 'upslop', 'cycling']

for i in range(len(labels)):
   filename = '/home/mikakohayashi/Desktop/1325/newcalitemp/' + labels[i] + 'cali.txt'
   with open(filename, 'rb') as f:
        files.append(pickle.load(f))
        


raws = []

for i in range(len(labels)):
    filename = '/home/mikakohayashi/Desktop/1325/newcalitemp/' + labels[i] + 'exp.txt'    
    raws.append(pd.read_pickle(filename))
        
for i in range(len(files)):
    files[i].sort(key=len)
    raws[i].sort(key=len)
    for j in range(len(files[i])):
#        print(len(files[i][j]), len(raws[i][j]))
        if len(files[i][j]) != len(raws[i][j]):
            print(str(i) + '' + str(j) + ' No')
    print('==================================================')
#%%

b, a = sig.butter(2, 0.2)
print(b,a)

def filtdata(calibdata_list):
    filtdata_list = []

    for i in range(len(calibdata_list)):
        filtdata_list.append(np.zeros((len(calibdata_list[i]), 14)))
        for j in range(0,14):
            filtdata_list[i][:,j]= sig.filtfilt(b,a,calibdata_list[i][:,j])

    for i in range(len(filtdata_list)):
        # remove the negative value
        filtdata_list[i][filtdata_list[i]<0] = 0
 
        
    return filtdata_list

filt_list = []

for i in range(len(files)):
    filt_list.append(filtdata(files[i])) 
    
#%%

def cutWindow(data, wl, step):

#    data_list_reshaped = []
    data_window_lists = []

    j = 0
    
    while j < len(data) - wl:
        window = data[j : j + wl]
        data_window_lists.append(window)
        j += step
    
    return data_window_lists

windowtotal = []    
labeltotal = []
for r in filt_list:
    for each in r:
        temp = cutWindow(each,1000,500)
        for t in temp:
            windowtotal.append(t)
for r in raws:
    for each in r:
        temp = cutWindow(each,1000,500)
        for t in temp:
            labeltotal.append(t.Activity)
#%%

"Make the giant feature list"
feature_all = list()
for each in windowtotal:
    featureforone = []
    window = each
    temp = sensorgeneral(window)
    for i in temp:
        for j in i:
            featureforone.append(j)
    temp = inerpeakinfo(window)
    for i in temp:
        for j in i:
            featureforone.append(j)
    temp = instepfeature(window)            #!!!!!!!!!!!!this will needa big midification on calibration
    featureforone.append(temp)
    temp = anterposter(window)
    for i in temp:
        featureforone.append(i)
    temp = lattomid(window)
    for i in temp:
        featureforone.append(i)
    temp = overlappingrate(window)
    featureforone.append(temp)
    temp = fft(window)
    for i in temp:
        featureforone.append(i)


    feature_all.append(featureforone)
    
label_all = list()
for each in labeltotal:
    label_all.append(np.array(each)[0])
#%%
df_feature = pd.DataFrame(feature_all,columns=['aveL1', 'aveL2','aveL3', 'aveL4','aveL5', 'aveL6','aveL7',
                                               'aveR1', 'aveR2','aveR3', 'aveR4','aveR5', 'aveR6','aveR7',
                                               'stdL1', 'stdL2','stdL3', 'stdL4','stdL5', 'stdL6','stdL7',
                                               'stdR1', 'stdR2','stdR3', 'stdR4','stdR5', 'stdR6','stdR7',
                                               'midL1', 'midL2','midL3', 'midL4','midL5', 'midL6','midL7',
                                               'midR1', 'midR2','midR3', 'midR4','midR5', 'midR6','midR7',
                                               'maxL1', 'maxL2','maxL3', 'maxL4','maxL5', 'maxL6','maxL7',
                                               'maxR1', 'maxR2','maxR3', 'maxR4','maxR5', 'maxR6','maxR7',
                                       'peaknumL1','peakdisaveL1','peakdisstdL1','peakmgaveL1','peakmgstdL1','peakwthaveL1','peakwthstdL1',
                                       'peaknumL2','peakdisaveL2','peakdisstdL2','peakmgaveL2','peakmgstdL2','peakwthaveL2','peakwthstdL2',
                                       'peaknumL3','peakdisaveL3','peakdisstdL3','peakmgaveL3','peakmgstdL3','peakwthaveL3','peakwthstdL3',
                                       'peaknumL4','peakdisaveL4','peakdisstdL4','peakmgaveL4','peakmgstdL4','peakwthaveL4','peakwthstdL4',
                                       'peaknumL5','peakdisaveL5','peakdisstdL5','peakmgaveL5','peakmgstdL5','peakwthaveL5','peakwthstdL5',
                                       'peaknumL6','peakdisaveL6','peakdisstdL6','peakmgaveL6','peakmgstdL6','peakwthaveL6','peakwthstdL6',
                                       'peaknumL7','peakdisaveL7','peakdisstdL7','peakmgaveL7','peakmgstdL7','peakwthaveL7','peakwthstdL7',
                                       'peaknumR1','peakdisaveR1','peakdisstdR1','peakmgaveR1','peakmgstdR1','peakwthaveR1','peakwthstdR1',
                                       'peaknumR2','peakdisaveR2','peakdisstdR2','peakmgaveR2','peakmgstdR2','peakwthaveR2','peakwthstdR2',
                                       'peaknumR3','peakdisaveR3','peakdisstdR3','peakmgaveR3','peakmgstdR3','peakwthaveR3','peakwthstdR3',
                                       'peaknumR4','peakdisaveR4','peakdisstdR4','peakmgaveR4','peakmgstdR4','peakwthaveR4','peakwthstdR4',
                                       'peaknumR5','peakdisaveR5','peakdisstdR5','peakmgaveR5','peakmgstdR5','peakwthaveR5','peakwthstdR5',
                                       'peaknumR6','peakdisaveR6','peakdisstdR6','peakmgaveR6','peakmgstdR6','peakwthaveR6','peakwthstdR6',
                                       'peaknumR7','peakdisaveR7','peakdisstdR7','peakmgaveR7','peakmgstdR7','peakwthaveR7','peakwthstdR7',
                                       'inerstepinterial','APdiff','APcoraL','APcoraR','LMdiff','LMcoraL','LMcoraR','overlap','fftdc','freq_spectrumave','freq_spectrumstd','entropy','energy'])

df_label =  pd.DataFrame(label_all,columns = ['Activity'])
sample = pd.concat([df_feature, df_label], axis=1, sort=False)
#%%
sample['Activity'][sample['Activity']=='walkF'] = 'walk'
sample['Activity'][sample['Activity']=='walkN'] = 'walk'
sample['Activity'][sample['Activity']=='jog'] = 'run'
#%%

#sample = sample[['aveL1','stdL1','midL1','maxL1','peaknumL1','peakdisaveL1','peakdisstdL1','peakmgaveL1','peakmgstdL1','peakwthaveL1','peakwthstdL1','Activity']]

#%%
labels = ['upstairs', 'downstairs', 'nonlocal', 'run', 'sitting', 'standing', 'upslop', 'cycling','walk']

sample = sample.fillna(0)
training_sample_rate = 0.3      #""""""""""Changeable""""""""""!!!!!!!!!!!!!!!!!!!!!!!

n_ft = sample.shape[1]

X_train = np.array([], dtype=np.int64).reshape(0,n_ft-2)
X_test = X_train.copy()
y_train = []
y_test = []

for each in labels:
    sample_one_act = sample.loc[sample['Activity'] == each]
    n_trn_smpls = int(training_sample_rate*len(sample_one_act))
    
    trn = sample_one_act.iloc[:n_trn_smpls]
    tst = sample_one_act.iloc[n_trn_smpls:]
    
    X_train = np.concatenate((X_train, trn.iloc[:,1:-1].values), axis=0)
    X_test = np.concatenate((X_test, tst.iloc[:,1:-1].values), axis=0)
    y_train = np.concatenate((y_train, trn.iloc[:,-1].values), axis=0)
    y_test = np.concatenate((y_test, tst.iloc[:,-1].values), axis=0)
#%%
'''keep this important 
#feature.loc[feature['Activiy'] == 'run'] #I will need this
    
#feature_list = feature.iloc[:,1:-1]  
#
#label_list = feature.iloc[:,-1]  
#train_samples = int(training_sample_rate*len(feature))
#X_train = feature_list[:train_samples]  #How to ditinguish subs
#X_test = feature_list[train_samples:]
#y_train = label_list[:train_samples]
#y_test = label_list[train_samples:]
'''

myRandomSeed = 0 #np.random.randint(1000000)
print("seed: ", myRandomSeed)
rf = RandomForestClassifier(n_estimators=100, random_state=myRandomSeed, verbose=1, min_samples_split=2)     #?????? try to decrease the numbers of trees, too much 
rf.fit(X_train, y_train)

print(rf.feature_importances_)
y_pred = rf.predict(X_test)

score_rf = accuracy_score(y_test, y_pred)

#%%
'''
Draw the confusion matrix
'''


y_true = y_test.copy()

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = unique_labels(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=['upstairs', 'downstairs', 'jog', 'nonlocal', 'run', 'sit', 'stand', 'walkF', 'walkN', 'upslop', 'cycling','walk'],
                      title='Confusion matrix without normalization')

#Plot normalized confusion matrix
plot_confusion_matrix(y_test, y_pred, classes=['upstairs', 'downstairs', 'jog', 'nonlocal', 'run', 'sit', 'stand', 'walkF', 'walkN', 'upslop', 'cycling','walk'], normalize=True,
                      title='Normalized confusion matrix')

plt.show()

print("--- %s seconds ---" % (time.time() - start_time))
